__NUXT_JSONP__("/articles/ai-ml-study-ch4", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd){return {data:[{article:{slug:"ai-ml-study-ch4",description:"모델을 훈련시키는 방법을 배워봅니다.",title:"ML Chap 4. Training Models",category:"Data-Science",author:ag,toc:[{id:au,depth:av,text:aw},{id:ax,depth:Y,text:ay},{id:az,depth:Y,text:aA},{id:aB,depth:av,text:aC},{id:aD,depth:Y,text:aE},{id:aF,depth:Y,text:aG}],body:{type:"root",children:[{type:b,tag:"h1",props:{id:"training-models"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#training-models",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Training Models"}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Hands-on-Machine-Learning Chap4"}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"먼저 챕터 4에서는 Linear regression을 사용하여 모델을 훈련시키는 두 가지 방법을 소개합니다.\n(1) The Normal Equation\n(2) Gradient Descent(GD)"}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"다음으로 Polynomial regression을 살펴봅니다.\nPolynomial regression은 Linear regression에 비해 parameter가 많아 복잡하고 과대적합되는 경향이 있습니다. 그래서 과대적합을 줄일 수 있는 몇가지 기술을 알아볼 예정입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"마지막으로 classification의 대표적인 모델인 Logistic regression과 Softmax regression을 살펴봅니다."}]},{type:a,value:f},{type:b,tag:aH,props:{id:au},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#regression-case",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:aw}]},{type:a,value:f},{type:b,tag:Z,props:{id:ax},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#linear-regression",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:b,tag:M,props:{},children:[{type:a,value:ay}]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Linear regression model prediction",src:"\u002Fai-ml-study-ch4\u002F1.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Linear regression은 기본적으로 위의 식처럼 가중치가 있는 측성들과 bias term이라고 불리는 상수의 합으로 만들어집니다.\n여기서 y hat은 예측된 값, n은 특성의 개수, x_i는 i번째 값, theta_j는 j번째 모델의 parameter입니다."}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:S}]},{type:a,value:aI},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:aJ}]},{type:a,value:aK},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:K}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:N}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aL}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aj}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:ak},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:N}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"linear data",src:"\u002Fai-ml-study-ch4\u002F2.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"챕터 2에서 사용했던 것 처럼 Linear regression이 얼마나 잘 적합한지 확인하려면 RMSE 또는 MSE가 작아지도록 하는 theta를 찾으면 됩니다. (실전에서는 RMSE보다 MSE가 더 많이 쓰이지만 둘다 같은 결과를 만듦.)"}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"이번 챕터에서 cost function이 많이 등장하니 다시 한 번 cost function이 무엇이었는지 확인합시다."}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"cost function(비용함수) : 모델이 얼마나 나쁜지 측정하는 함수\nutility function(효용함수) : 모델이 얼마나 좋은지 측정하는 함수"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Linear regression에서 MSE는 값이 커질수록 모델이 좋지 않은 것이므로 cost function이라고 볼 수 있습니다."}]},{type:a,value:f},{type:b,tag:L,props:{id:"1-the-normal-equation"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#1-the-normal-equation",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"(1) "},{type:b,tag:M,props:{},children:[{type:a,value:"The normal equation"}]}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"the Normal Equation : cost function를 최소화하는 theta값은 찾을 수 있는 수학적인 식"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"cost function를 최소화하는 theta값을 찾는 방법에는 closed-form solution이 있습니다. 이것은 수학적인 식을 이용하여 직접적으로 값을 찾는 방식으로 그 식을 the Normal Equation이라고 합니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Normal Equation",src:"\u002Fai-ml-study-ch4\u002F3.png"},children:[]},{type:a,value:"\ntheta hat은 cost function를 최소화하는 theta값이고, y는 y1부터 ym까지 포함하고 있는 target value의 vector입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"임의의 linear 모델을 만든 후 Normal equation을 이용하여 theta hat을 계산해 보겠습니다."}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:b,tag:c,props:{className:[d,O]},children:[{type:a,value:"#임의의 linear 모델"}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:S}]},{type:a,value:aI},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:aJ}]},{type:a,value:aK},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:K}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:N}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aL}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aj}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:ak},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:N}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:a,value:"X_b "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"c_"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:a,value:"np"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"ones"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:N}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:aM},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,O]},children:[{type:a,value:"# add x0 = 1 to each instance "}]},{type:a,value:"\ntheta_best "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"linalg"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"inv"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"y"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:L,props:{id:"2-gradient-descent"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#2-gradient-descent",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"(2) "},{type:b,tag:M,props:{},children:[{type:a,value:aN}]}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Gradient Descent(우리 말로는 경사하강법) : 가장 일반적인 최적화 알고리즘"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"GD는 문제의 최적화 솔루션을 찾을 때 사용되는데, GD의 아이디어는 비용함수를 최소화하기 위해 prameter를 수정하는 것입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"parameter vecter theta에 대한 비용함수의 현재 GD를 계산하고 GD가 감소하는 방향으로 진행합니다. 그러다 GD가 0이 되면 최솟값에 도달한 것입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:aN,src:"\u002Fai-ml-study-ch4\u002F4.png"},children:[]}]},{type:a,value:f},{type:b,tag:U,props:{},children:[{type:a,value:f},{type:b,tag:V,props:{},children:[{type:a,value:"learning rate"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Gradient Descent에서 가장 중요한 parameter : step의 크기"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"step의 크기는 learning rate hyperparameter로 결정됩니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"만약 learning rate가 너무 작으면 최솟값에 도달하기 까지 시행횟수가 너무 많아 오래 걸립니다.\n"},{type:b,tag:o,props:{alt:"Gradient Descent step1",src:"\u002Fai-ml-study-ch4\u002F5.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"반면 learning rate가 너무 크면 최솟값을 가로질러 반대편으로 건너뛰게 되어 이전보다 더 최솟값에 멀어지게 될 수 있고, 이는 발산하게 만들어 솔루션을 찾지 못하게 합니다.\n"},{type:b,tag:o,props:{alt:"Gradient Descent step2",src:"\u002Fai-ml-study-ch4\u002F6.png"},children:[]}]},{type:a,value:f},{type:b,tag:U,props:{},children:[{type:a,value:f},{type:b,tag:V,props:{},children:[{type:a,value:"Gradient Descent의 안 좋은 예"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"모든 비용함수가 매끈하지 않기 때문에 최솟값으로 수렴하기 어려운 경우가 있을 수 있습니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"알고리즘이 아래 그림의 왼쪽에서 시작하면 전역최솟값보다 덜 좋은 지역최솟값에 수렴하게 됩니다.\n반면 오른쪽에서 시작하면 완만한 지역을 지나는데 시간이 오래 걸리고 일찍 멈추게 되어 전역 최솟값에 도달하지 못합니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Gradient Descent의 문제",src:"\u002Fai-ml-study-ch4\u002F7.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Linear regression에서 MSE 비용함수는 위의 문제가 발생하지 않는 볼록함수이기 때문에, 즉, 지역 최솟값이 없이 하나의 전역 최솟값만 있고 기울기가 갑자기 변하지 않는 연속함수 이기 때문에, Grdient Descent가 전역최솟값에 도달할 수 있음을 보장됩니다."}]},{type:a,value:f},{type:b,tag:U,props:{},children:[{type:a,value:f},{type:b,tag:V,props:{},children:[{type:a,value:"scale"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Gradient Descent를 사용할 때는 scale이 다르면 수렴하는데 더 오래 걸리므로 반드시 모든 특성이 같은 scale을 갖도록 해야합니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"sclae을 하기 전 후",src:"\u002Fai-ml-study-ch4\u002F8.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"위의 그림을 보면 왼쪽은 알고리즘이 최솟값으로 바로 진행하고 오른쪽은 돌아서 진행합니다."}]},{type:a,value:f},{type:b,tag:W,props:{id:"batch-gradient-descent"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#batch-gradient-descent",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Batch Gradient Descent"}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Batch Gradient Descent : 전체 데이터를 하나의 batch로 묶어 학습시키는 Gradient Descent"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:a,value:"eta "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:al}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,O]},children:[{type:a,value:"# learning rate "}]},{type:a,value:"\nn_iterations "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:"1000"}]},{type:a,value:" \nm "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:N}]},{type:a,value:aO},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,O]},children:[{type:a,value:aP}]},{type:a,value:aQ},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:am}]},{type:a,value:" iteration "},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:an}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,ao]},children:[{type:a,value:ap}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"n_iterations"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:Q}]},{type:a,value:" \n  gradients "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:aR}]},{type:a,value:aS},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:aT},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aU},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:X}]},{type:a,value:R},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:" \n  theta "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:aV},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:X}]},{type:a,value:aW},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:aX}]}]}]},{type:a,value:f},{type:b,tag:W,props:{id:"stochastic-gradient-descent"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#stochastic-gradient-descent",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Stochastic Gradient Descent"}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Stochastic Gradient Descent : 매 step에서 한 개의 샘플을 무작위 선택하고, 그 하나의 샘플에 대한 GD를 계산하는 방법"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:a,value:"n_epochs "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aY}]},{type:a,value:" t0"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:" t1 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:"5"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aY}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,O]},children:[{type:a,value:"# learning schedule hyperparameters "}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:"def"}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,"function"]},children:[{type:a,value:"learning_schedule"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"t"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:Q}]},{type:a,value:" \n"},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:"return"}]},{type:a,value:" t0 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:aR}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"t "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:" t1"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:aO},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,O]},children:[{type:a,value:aP}]},{type:a,value:aQ},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:am}]},{type:a,value:" epoch "},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:an}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,ao]},children:[{type:a,value:ap}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"n_epochs"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:Q}]},{type:a,value:" \n  "},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:am}]},{type:a,value:" i "},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:an}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,ao]},children:[{type:a,value:ap}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aa},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:Q}]},{type:a,value:" \n    random_index "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"randint"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aa},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:" \n    xi "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:aT},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:Q}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:a,value:" \n    yi "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:R},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:Q}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:a,value:" \n    gradients "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:K}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:" xi"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"xi"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aU},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:X}]},{type:a,value:" yi"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:" \n    eta "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" learning_schedule"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"epoch "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:" m "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:" i"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:" \n    theta "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:aV},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:X}]},{type:a,value:aW},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:aX}]}]}]},{type:a,value:f},{type:b,tag:W,props:{id:"mini-batch-gradient-descent"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#mini-batch-gradient-descent",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Mini-batch Gradient Descent"}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Mini-batch Gradient Descent : mini-batch라 부르는 임의의 작은 샘플 세트에 대해 GD 계산하는 방법"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:aZ,props:{},children:[]},{type:a,value:f},{type:b,tag:Z,props:{id:az},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#polynomial-regression",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:b,tag:M,props:{},children:[{type:a,value:aA}]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Polynomial Regression은 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 추가된 특성을 포함한 데이터 세트에 linear model을 훈련시키는 방법입니다. nonlinear data를 학습시키는데 사용합니다."}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:a,value:aS},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:N}]},{type:a,value:" \nX "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:"6"}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aa},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:X}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aj}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:a_}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:x}]},{type:a,value:aM},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:"**"}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:K}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:ak},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:K}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:z}]},{type:a,value:y},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:J},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aa},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"nonlinear and noisy dataset",src:"\u002Fai-ml-study-ch4\u002F9.png"},children:[]}]},{type:a,value:f},{type:b,tag:L,props:{id:"learning-curves"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#learning-curves",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Learning curves"}]},{type:a,value:f},{type:b,tag:L,props:{id:"regularized-linear-models"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#regularized-linear-models",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Regularized Linear Models"}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"과대적합을 줄이기 위해 좋은 방법은 모델을 regulized하는 것입니다.\nPolynomial Regression은 차수를 감소시킴으로써, Linear Regression은 모델의 가중치를 제한함으로써 모델을 regulized합니다.\n가중치를 제한 하는 방법으로 Ridge, Lasso, Elasitc Net을 소개합니다!"}]},{type:a,value:f},{type:b,tag:W,props:{id:"ridge-regression--lasso-regression"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#ridge-regression--lasso-regression",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:b,tag:M,props:{},children:[{type:a,value:"Ridge Regression \u002F Lasso Regression"}]}]},{type:a,value:f},{type:b,tag:U,props:{},children:[{type:a,value:f},{type:b,tag:V,props:{},children:[{type:b,tag:M,props:{},children:[{type:a,value:"Ridge Regression"}]}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Ridge Regression : Linear regression 모델에 L2 regularization이 추가된 모델"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Ridge regression의 cost function",src:"\u002Fai-ml-study-ch4\u002F10.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Ridge는 특성의 중요도에 따라 얼마나 많이 규제할지, 가중치를 조절합니다."}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:aq}]},{type:a,value:ar},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:as},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:S}]},{type:a,value:" Ridge \nridge_reg "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" Ridge"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:at},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:" solver"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,a$]},children:[{type:a,value:"\"cholesky\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:" \nridge_reg"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:R},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"  \n\nridge_reg"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:af}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:" \n\n"},{type:b,tag:c,props:{className:[d,O]},children:[{type:a,value:"#Stochasitc Gradient Descent"}]},{type:a,value:"\nsgd_reg "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" SGDRegressor"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"penalty"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,a$]},children:[{type:a,value:"\"l2\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:" \nsgd_reg"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:R},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"ravel"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"\nsgd_reg"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:af}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:U,props:{},children:[{type:a,value:f},{type:b,tag:V,props:{},children:[{type:b,tag:M,props:{},children:[{type:a,value:"Lasso Regression"}]}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Lasso Regression : Linear regression 모델에 L1 regularization이 추가된 모델"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Lasso regression의 cost function",src:"\u002Fai-ml-study-ch4\u002F11.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Lasso는 덜 중요한 특성의 가중치를 제거, 즉 가중치를 0으로 만듭니다."}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:aq}]},{type:a,value:ar},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:as},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:S}]},{type:a,value:" Lasso \nlasso_reg "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" Lasso"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:at},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:al}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:ba},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:R},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:ba},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:af}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:W,props:{id:"elastic-net"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#elastic-net",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Elastic Net"}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Elastic Net : Regression 모델에 L1과 L2 regularization이 결합된 모델"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Ridge와 Lasso를 절충한 것이 Elastic Net입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Elastic Net의 cost function",src:"\u002Fai-ml-study-ch4\u002F12.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Elastic Net의 식에서 규제항을 보면 Ridge와 Lasso 식에 보이던 규제항의 합이 규제항이 된다는 것을 확인할 수 있습니다."}]},{type:a,value:f},{type:b,tag:D,props:{className:[E]},children:[{type:b,tag:F,props:{className:[G,H]},children:[{type:b,tag:I,props:{},children:[{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:aq}]},{type:a,value:ar},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:as},{type:b,tag:c,props:{className:[d,u]},children:[{type:a,value:S}]},{type:a,value:" ElasticNet \nelastic_net "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" ElasticNet"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:at},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:al}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:" l1_ratio"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:a_}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:bb},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:v}]},{type:a,value:R},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:bb},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:af}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:C}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:A,props:{},children:[{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Ridge를 기본으로 쓰되 중요한 특성이 적다고 의심되면 Lasso나 Elastic Net을 사용하는 것이 좋습니다. 특성 수가 훈련 샘플 수보다 많거나 특성 몇개가 강하게 연관되어 있으면 Lasso보다 Elastic Net을 사용하는 것이 좋습니다."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:L,props:{id:"early-stopping"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#early-stopping",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:bc}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"GD와 같은 반복적인 학습 알고리즘을 규제하는 다른 방식은 validation error가 최솟값에 도달하면 훈련을 중지시키는 것입니다. 이것을 Early sttoping이라고 합니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:bc,src:"\u002Fai-ml-study-ch4\u002F13.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"epoch가 진행됨에 따라 알고리즘이 점차 학습되어 훈련세트에 대한 예측 에러와 검증 세트에 대한 예측 에러가 줄어듭니다. 그러나 감소하던 validation error가 멈추었다가 다시 상승하는데, 이는 모델이 훈련데이터에 과대적합되기 시작했음을 알려주는 것입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"그래서 Early stopping이 최솟값에 멈추도록 하는 것은 과대적합을 막아줍니다."}]},{type:a,value:f},{type:b,tag:aZ,props:{},children:[]},{type:a,value:f},{type:b,tag:aH,props:{id:aB},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#classification-case",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:aC}]},{type:a,value:f},{type:b,tag:Z,props:{id:aD},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#logistic-regression",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:aE}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Logistic Regression은 샘플이 특정 클래스에 속할 확률을 추정하는데 사용합니다.\n샘플이 특정 클래스에 속해야 하므로 타겟 변수가 categorical인 경우 사용할 수 있습니다.\nex) 메일이 스팸일 확률"}]},{type:a,value:f},{type:b,tag:L,props:{id:"estimationg-probabilities"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#estimationg-probabilities",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Estimationg Probabilities"}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"앞에서 Linear Regression은 input feature의 가중치 합을 계산했습니다. Logistic Regression도 동일하게 계산하는데, 대신 Linear regression처럼 바로 결과를 출력하지 않고 결과값의 logistic을 출력합니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Logistic probability",src:"\u002Fai-ml-study-ch4\u002F14.png"},children:[]},{type:a,value:"\n여기서 p hat은 샘플 x가 양성 클래스에 속할 확률입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"logistic은 0과 1 사이의 값을 출력하는 sigmoid function(즉, S-shape)입니다. 이는 다음 식과 그림에서 확인할 수 있습니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Logistic function equation",src:"\u002Fai-ml-study-ch4\u002F15.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Logistic function",src:"\u002Fai-ml-study-ch4\u002F16.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"p hat이 샘플이 양성 클래스에 속할 확률이므로, p hat을 추정만 하게 되면  prediction y hat도 쉽게 찾을 수 있습니다. y hat을 p hat이 0.5보다 크거나 같으면 1, 작으면 0으로 정의하면 되기 때문입니다."}]},{type:a,value:f},{type:b,tag:L,props:{id:"training-and-cost-function"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#training-and-cost-function",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Training and Cost Function"}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"앞서 언급했듯이 Logistic Regression은 샘플이 특정 클래스에 속할 확률을 추정하는데 사용합니다. Logistic Regression의 목적은 양성 샘플에 대해서는 높은 확률을 추정하고 음성 샘플에 대해서는 낮은 확률을 추정하는 모델의 parameter vector인 theta를 찾는 것입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"하나의 훈련 샘플에 대한 cost function은 다음과 같습니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"하나의 훈련 샘플에 대한 cost function",src:"\u002Fai-ml-study-ch4\u002F17.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"t가 0에 가까워지면 -log(t)가 매우 커지고 1에 가까워지면 0에 가까워지기 때문에 위에 첨부된 cost function은 양성 샘플을 0에 가까운 확률로 추정하거나 음성 샘플을 1에 가까운 확률로 추정하면 비용이 증가합니다. 반대로, 음성 샘플을 0에 가까운 확률로 추정하거나 양성 샘플을 1에 가까운 확률로 추정하면 비용은 감소합니다.\n따라서 이는 적절한 cost function이라고 할 수 있습니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"하나의 훈련 샘플이 아니라 전체 훈련 세트에 대한 cost function은 아래와 같습니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"Logistic Regression의 cost function",src:"\u002Fai-ml-study-ch4\u002F18.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"이는 모든 훈련 샘플의 비용의 평균인데 log loss(로그 손실)라고도 부릅니다. 이 cost function의 최솟값을 계산하는 방법이나 해는 없습니다. 하지만 학습률이 너무 크지 않고 충분한 시간을 들인다면 ,이 cost function이 볼록 함수이므로 Gradient Descent가 전역 최솟값을 찾는 것을 보장합니다."}]},{type:a,value:f},{type:b,tag:L,props:{id:"decision-boundaries"},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#decision-boundaries",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:"Decision Boundaries"}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"두 클래스의 확률이 똑같이 50%가 되는 근방에서 Decision Boundary가 만들어집니다."}]},{type:a,value:f},{type:b,tag:Z,props:{id:aF},children:[{type:b,tag:p,props:{ariaHidden:q,href:"#softmax-regression",tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:aG}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Logistic Regression은 여러 개의 이진 분류기를 훈련시켜 연결하지 않고 직접 다중 클래스를 지원하도록 일반화될 수 있는데, 이를 Softmax Regression또는 Multinomial Logistic Regession이라고 합니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"샘플 x가 주어지면 Softmax Regression이 각 클래스 k에 대한 점수를 계산하고,\n"},{type:b,tag:o,props:{alt:"Softmax score",src:"\u002Fai-ml-study-ch4\u002F19.png"},children:[]},{type:a,value:"\n그 점수에 Softmax fuction를 적용하여\n"},{type:b,tag:o,props:{alt:"Softmax function",src:"\u002Fai-ml-study-ch4\u002F20.png"},children:[]},{type:a,value:"\n각 클래스의 확률을 추정하는 방식입니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"Logistic Regression classifier과 마찬가지로 추정 확률이 가장 높은, 가장 높은 점수를 가진 클래스를 선택합니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"모델이 타겟 클래스에 대해서 높은 확률을 추정하도록 만드는 것이 목적이기 때문에 Softmax Regression에서도 cost function을 사용합니다. cost function은 다음과 같습니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:b,tag:o,props:{alt:"cross entropy cost function",src:"\u002Fai-ml-study-ch4\u002F21.png"},children:[]}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"cross entropy cost function이라고 하는 위의 식을 최소화하여 타겟 클래스에 대해 낮은 확률을 예측하는 모델을 억제시키도록 합니다. 또 추정된 클래스의 확률이 타겟 클래스에 얼마나 잘 맞는지 측정하는데 사용합니다."}]},{type:a,value:f},{type:b,tag:g,props:{},children:[{type:a,value:"클래스가 2개일 때, cross entropy cost function은 Logistic Regression의 cost function과 같습니다."}]}]},dir:"\u002Farticles",path:"\u002Farticles\u002Fai-ml-study-ch4",extension:".md",createdAt:bd,updatedAt:bd},prev:{slug:"javascript-study-asynchronous",title:"Javascript 비동기 메커니즘 -1"},next:{slug:"ai-ml-study-ch2-2",title:"ML Chap 2. Machine Learning Project-2"},member:[{slug:"hyewon",name:ag,description:"중앙대학교 GDSC member로 수학과, 금융수학 전공중입니다.",role:"Member",img:"hyewon.JPG",dir:"\u002Fmembers",path:"\u002Fmembers\u002Fhyewon",extension:".yaml",createdAt:"2022-11-03T10:04:04.000Z",updatedAt:"2022-11-03T17:31:53.000Z"}],authorName:ag}],fetch:{},mutations:void 0}}("text","element","span","token","punctuation","\n","p","operator"," ","number",".","(",")","=","img","a","true",-1,"icon","icon-link","keyword",",","1","*"," np","+","blockquote","[","]","div","nuxt-content-highlight","pre","language-python","line-numbers","code","random","2","h4","strong","100","comment","dot",":"," y","import","randn","ul","li","h5","-",3,"h3","X_b","T","m","random_index","fit","X","predict","1.5","HyeWon Lee","rand"," \ny ","3"," X ","0.1","for","in","builtin","range","from"," sklearn","linear_model ","alpha","regression-case",2,"Regression case","linear-regression","Linear Regression","polynomial-regression","Polynomial Regression","classification-case","Classification case","logistic-regression","Logistic Regression","softmax-regression","Softmax Regression","h2"," numpy ","as"," np \nX ","4"," X","Gradient Descent"," \n\ntheta ","# random initialization ","\n\n","\u002F","m "," X_b","theta"," theta "," eta "," gradients\n\ntheta\n","50","hr","0.5","string"," \nlasso_reg"," \nelastic_net","Early stopping","2022-11-27T12:22:03.000Z")));